\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment


\begin{document}
\title{The Higgs boson Machine Learning Challenge}

\author{
  Emanuele Nevali, Matteo Suez, Leonardo Bruno Trentini\\
  \textit{Machine Learning and Optimization Laboratory, EPFL, Switzerland}
}

\maketitle

\begin{abstract}
In this project, carried out as part of the 2022 Machine Learnng course at EPFL, we applied machine learning techniques to actual CERN particle accelerator data to recreate the process of
“discovering” the Higgs particle.\\
\end{abstract}

\section{Introduction}
Rarely, collisions between protons at high speed can produce a Higgs boson. Since the Higgs boson decays rapidly into other particles, it is not possible to observe it directly, but rather measure its “decay signature”, or the products that result from its decay process. The aim of this project is to estimate the likelihood that a given event’s signature was the result of a Higgs boson (signal) or some other process/particle (background).\\
After analysis and preprocessing of the dataset, we introduce the model built with the 6 demanded algorithms and we explain how we managed to choose the best hyperparameters. In conclusion, we present our results.

\section{Data analysis and preprocessing}

\subsection{Exploratory data analysis}
Our dataset contains 250000 points for training and 568238
for testing with their corresponding binary label (“s” for “signal” and “b” for “background”, which have to be predicted for the test set values). All of these points are characterized by 30 different features. \\
\vspace{-0.8cm}\\

\subsection{Categorization}
First of all, we notice that there is only one categorical feature, i.e. \textit{Pri jet num}: it shows the number of jets (see \cite{higgs} for a physical explanation), and it is an integer that ranges from 0 to 3. \\
We noticed that some features are meaningless for some values of jets, therefore we have split the dataset into 4 subclasses, each one characterized by a different \textit{Pri jet num}.\\
\vspace{-0.8cm}\\

\subsection{Missing Values}
Thanks to the documentation, we noticed that each “-999” value
in the dataset consists in a missing value: we decided to replace all of them with the median of the feature. In particular, we chose to use the median instead of the mean because of its nature of robust estimator to outliers.\\
Furthermore, for subclasses marked by \textit{Pri jet num} $0$ and $1$ we decided not to consider columns with too many missing values, in order to avoid singular matrices.\\
\vspace{-0.8cm}\\

\subsection{Absolute transform of Symmetrical Features}
For features whose distribution is symmetric around 0 and
for which the proportion of categories is not too even (in particular, features in columns [14, 17, 23, 26]) , we calculated and replaced with the absolute value.\\
This increases the gap between the two categories, and makes them better distinguishable by the model. \\
\vspace{-0.8cm}\\

\subsection{Outliers erasure}
After a search for outliers in the data, we set $\alpha = 0.1$ and decided to edge the extreme values of each feature to the $\alpha$-quantile (for the lower tail) and to the $(1-\alpha)$-quantile (for the upper tail).\\
\vspace{-0.8cm}\\

\subsection{Angles}
We noticed that some of the features represents measurements of angles. For this reason, in order to adapt our model to the periodicity, we tried to replace these columns with sine and cosine transformations. However, only the latter appeared to be effective, and it was the only one kept in the final model.\\
\vspace{-0.8cm}\\

\subsection{Correlated features}
We observed that the empirical distribution of some of the features was too similar, index of an high correlation between them. By not considering these features, we slightly improved our performance.\\
\vspace{-0.8cm}\\

\subsection{Heavy-tailed features}
Noticing that many of the features were heavy-tailed, we computed for these a logaritmic transformation: $x_{k}^{*} = log(1+x_k)$ (where $k$ is the feature column). These transformation  helps in reducing or removing the skewness of our original data.\\
\vspace{-0.8cm}\\


\subsection{Standardization}
In order to ensure a better functioning of our algorithms, we decided to standardize the data by subtracting from each feature its mean and dividing by its standard deviation: $Z=\frac{X-\mu}{\sigma}$. The same process was applied both to the train set and to the test set.\\
\vspace{-0.8cm}\\

\subsection{Polynomial Expansion}
We tried also to improve our predictions by expanding the features In particular, we augmented features by implementing a polynomial expansion for each column of the form $\phi:[X]\rightarrow [X,X^2,...,X^D]$.\\
The polynomial degree $D$ is one of the hyper-parameters of our model: we found its optimal value by performing a 4-fold cross validation on each subset.

\section{Methods}
After the pre-processing on the dataset, we managed to find a model for our problem by implementing six different methods:
\begin{itemize}
    \item \textbf{Gradient Descent}
    \item \textbf{Stochastic Gradient Descent}
    \item \textbf{Least squares} using normal equations
    \item \textbf{Ridge regression} using normal equations
    \item \textbf{Logistic regression} using gradient descent or stochastic gradient descent ($y \in {0,1}$)
    \item \textbf{Regularized logistic regression} using gradient descent or stochastic gradient descent ($y \in {0,1}$, with regularization term $ \lambda  \| \omega \|^2$)
\end{itemize}
All functions return: (w, loss), which is the last weight vector of the
method, and the corresponding loss value (or cost function).\\
\vspace{0.05cm}\\
Optimal parameters $\lambda$ (regularization to reduce overfitting), $\gamma$ (step size for gradient and stochastic gradient descent) and $D$ (degrees for polynomial expansion) were chosen through a 4-fold cross validation for each subset.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{report/lambdas.png}
    \caption{Grid search results for different values of $\lambda$ for each subset}
    \label{fig:lambdas}
\end{figure}\\


\section{Results}
Due to the fact that we were facing a binary classification problem, we first focused on the Regularized Logistic Regression. However, in the meanwhile we also carried out multiple tests with Least Squares and Ridge Regression, and remarkably the latter turned out to be the most effective method.\\
For this reason, we focused on calibrating Ridge Regression hyperparameters. We did it by using grid-search method and 4-fold cross-validation, looking for the best compromise between underfitting and overfitting. We adjusted the hyperparameters for
each of the four subclasses.\\
\vspace{0.05cm}\\
At the end, we obtained a model that achieved an accuracy of 0.831 on both AIcrowd and on local test set.
\section{Conclusions}
The best performance on the Higgs boson
challenge, with an accuracy on the test set of
0.831, was obtained with a Ridge Regression.\\
It is important to underline that data analysis, data cleaning and feature preprocessing played a key role in this task, enabling us to achieve massive improvements in model's accuracy.\\



\bibliographystyle{IEEEtran}
\bibliography{literature}

\end{document}