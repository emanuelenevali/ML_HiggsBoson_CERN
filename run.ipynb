{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Boson - ML Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from implementation import *\n",
    "from model_helpers import *\n",
    "from data_helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = { \n",
    "    'train' : 'data/train.csv',\n",
    "     'test' : 'data/test.csv',\n",
    "     'submission' : 'data/sample-submission.csv'\n",
    "        }\n",
    "\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(paths['train'], sub_sample=False)\n",
    "y_te, tx_te, ids_te = load_csv_data(paths['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_test = len(y_te)\n",
    "\n",
    "y_tr = y_tr[:, np.newaxis]\n",
    "y_pred = np.zeros(len_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into 4 different subsets depending on jet value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_tr = get_mask(tx_tr)\n",
    "mask_te = get_mask(tx_te)\n",
    "\n",
    "x_tr_subsamples = []\n",
    "y_tr_subsamples = []\n",
    "\n",
    "x_te_subsamples = []\n",
    "\n",
    "# create subsamples\n",
    "for i in range(4):\n",
    "    x_tr_subsamples.append(tx_tr[mask_tr[i]])\n",
    "    y_tr_subsamples.append(y_tr[mask_tr[i]])\n",
    "    x_te_subsamples.append(tx_te[mask_te[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process each subsample\n",
    "for j in range(4):\n",
    "    x_tr_subsamples[j] = pre_processing(x_tr_subsamples[j], j)\n",
    "    x_te_subsamples[j] = pre_processing(x_te_subsamples[j], j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree, gamma, function='',max_iters=1000):\n",
    "    \"\"\"\n",
    "    Return the loss of ridge regression for a fold corresponding to k_indices\n",
    "    \n",
    "    Args:\n",
    "        y:          shape=(N, 1)\n",
    "        x:          shape=(N, D)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold\n",
    "        lambda_:    scalar, used by ridge regression\n",
    "        degree:     scalar, used by build poly\n",
    "        gamma:      scalar, stepsize\n",
    "\n",
    "    Returns:\n",
    "        test loss: probability of predicting correct values\n",
    "    \"\"\"\n",
    "    \n",
    "    train_id = np.delete(k_indices, k, axis=0).ravel()\n",
    "    test_id = k_indices[k]\n",
    "    \n",
    "    x_tr, y_tr = x[train_id], y[train_id]\n",
    "    x_te, y_te = x[test_id], y[test_id]\n",
    "    \n",
    "    x_tr, x_te = build_poly(x_tr, degree), build_poly(x_te, degree)\n",
    "    \n",
    "    initial_w = np.zeros((x_tr.shape[1], 1))\n",
    "    \n",
    "    if function == 'LinearRegression':\n",
    "        \n",
    "        raise NotImplemented\n",
    "    \n",
    "    elif function == 'RidgeRegression':\n",
    "        \n",
    "        w, _ = ridge_regression(y_tr, x_tr, lambda_)\n",
    "    \n",
    "    elif function == 'LeastSquares':\n",
    "        \n",
    "        w, _ = least_squares(y_tr, x_tr)\n",
    "        \n",
    "    elif function == 'LogisticRegression':\n",
    "        \n",
    "        w, _ = logistic_regression(y_tr, x_tr, initial_w, max_iters, gamma)\n",
    "        \n",
    "    elif function == 'RegLogisticRegression':\n",
    "        \n",
    "        w, _ = reg_logistic_regression(y_tr, x_tr, lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    return (y_te == predict_labels(x_te, w)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_grid_search(txs, ys, func):\n",
    "    \"\"\"Runs cross validation on the data with different values of hyperparameters to compare accuracy\n",
    "    \n",
    "    Args:\n",
    "        txs: subsets of train dataset\n",
    "        ys:  labels of the different subsets\n",
    "        func: string, types of function\n",
    "        \n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    \n",
    "    seed = 51\n",
    "    k_fold = 4\n",
    "    \n",
    "    # Lambda: regularization parameter\n",
    "    lambdas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    \n",
    "    # Degree: feature augmentation\n",
    "    degrees = range(2, 8, 2)\n",
    "    \n",
    "    # Gamma: stepsize\n",
    "    gammas = [1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = []\n",
    "    for i in range(len(txs)):\n",
    "        k_indices.append(build_k_indices(ys[i].shape[0], k_fold, seed))\n",
    "        \n",
    "    print(\"Function: \" + func)\n",
    "        \n",
    "    # cross validation\n",
    "    for i in range(len(txs)):\n",
    "        \n",
    "        max_acc = 0\n",
    "        print(f\"*Set {i}\")\n",
    "        \n",
    "        for l in lambdas:\n",
    "            for d in degrees:\n",
    "                for g in gammas:\n",
    "                    \n",
    "                    pred_pcts = []\n",
    "                    \n",
    "                    for k in range(k_fold):\n",
    "                        pred_pct = cross_validation(ys[i], txs[i], k_indices[i], k, l, d, g, func)\n",
    "                        pred_pcts.append(pred_pct)\n",
    "                        \n",
    "                    pct = np.mean(pred_pcts)\n",
    "                    if pct > max_acc:\n",
    "                        max_acc = pct\n",
    "                        print(f\">>>>Set {i}/lamdba={l}/deg={d}/gamma={g}/ACC={np.around(pct, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: RidgeRegression\n",
      "*Set 0\n",
      ">>>>Set 0/lamdba=1e-05/deg=2/gamma=0.0001/ACC=0.834\n",
      ">>>>Set 0/lamdba=1e-05/deg=4/gamma=0.0001/ACC=0.842\n",
      ">>>>Set 0/lamdba=1e-05/deg=6/gamma=0.0001/ACC=0.843\n",
      ">>>>Set 0/lamdba=0.0001/deg=6/gamma=0.0001/ACC=0.843\n",
      "*Set 1\n",
      ">>>>Set 1/lamdba=1e-05/deg=2/gamma=0.0001/ACC=0.789\n",
      ">>>>Set 1/lamdba=1e-05/deg=4/gamma=0.0001/ACC=0.805\n",
      "*Set 2\n",
      ">>>>Set 2/lamdba=1e-05/deg=2/gamma=0.0001/ACC=0.817\n",
      ">>>>Set 2/lamdba=1e-05/deg=4/gamma=0.0001/ACC=0.829\n",
      ">>>>Set 2/lamdba=1e-05/deg=6/gamma=0.0001/ACC=0.829\n",
      "*Set 3\n",
      ">>>>Set 3/lamdba=1e-05/deg=2/gamma=0.0001/ACC=0.822\n",
      ">>>>Set 3/lamdba=1e-05/deg=4/gamma=0.0001/ACC=0.837\n",
      ">>>>Set 3/lamdba=0.0001/deg=4/gamma=0.0001/ACC=0.837\n",
      ">>>>Set 3/lamdba=0.001/deg=6/gamma=0.0001/ACC=0.838\n",
      "Function: LeastSquares\n",
      "*Set 0\n",
      ">>>>Set 0/lamdba=1e-05/deg=2/gamma=0.0001/ACC=0.834\n",
      ">>>>Set 0/lamdba=1e-05/deg=4/gamma=0.0001/ACC=0.842\n",
      ">>>>Set 0/lamdba=1e-05/deg=6/gamma=0.0001/ACC=0.842\n",
      "*Set 1\n",
      ">>>>Set 1/lamdba=1e-05/deg=2/gamma=0.0001/ACC=0.789\n",
      ">>>>Set 1/lamdba=1e-05/deg=4/gamma=0.0001/ACC=0.805\n",
      "*Set 2\n",
      ">>>>Set 2/lamdba=1e-05/deg=2/gamma=0.0001/ACC=0.817\n",
      ">>>>Set 2/lamdba=1e-05/deg=4/gamma=0.0001/ACC=0.829\n",
      ">>>>Set 2/lamdba=1e-05/deg=6/gamma=0.0001/ACC=0.829\n",
      "*Set 3\n",
      ">>>>Set 3/lamdba=1e-05/deg=2/gamma=0.0001/ACC=0.822\n",
      ">>>>Set 3/lamdba=1e-05/deg=4/gamma=0.0001/ACC=0.837\n",
      "Function: LogisticRegression\n",
      "*Set 0\n",
      ">>>>Set 0/lamdba=1e-05/deg=2/gamma=0.0001/ACC=0.745\n",
      ">>>>Set 0/lamdba=1e-05/deg=2/gamma=0.001/ACC=0.754\n",
      ">>>>Set 0/lamdba=1e-05/deg=2/gamma=0.01/ACC=0.825\n",
      ">>>>Set 0/lamdba=1e-05/deg=2/gamma=0.1/ACC=0.834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matteo\\Desktop\\GitHub\\ML_project1\\model_helpers.py:113: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.mean(y*np.log(sigmoid(tx_w)) + (1-y)*np.log(1-sigmoid(tx_w)))\n",
      "C:\\Users\\Matteo\\Desktop\\GitHub\\ML_project1\\model_helpers.py:113: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -np.mean(y*np.log(sigmoid(tx_w)) + (1-y)*np.log(1-sigmoid(tx_w)))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m functions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinearRegression\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRidgeRegression\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeastSquares\u001b[39m\u001b[38;5;124m'\u001b[39m, \\\n\u001b[0;32m      2\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogisticRegression\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegLogisticRegression\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m functions[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mcross_validation_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tr_subsamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_tr_subsamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36mcross_validation_grid_search\u001b[1;34m(txs, ys, func)\u001b[0m\n\u001b[0;32m     41\u001b[0m pred_pcts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k_fold):\n\u001b[1;32m---> 44\u001b[0m     pred_pct \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mys\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     pred_pcts\u001b[38;5;241m.\u001b[39mappend(pred_pct)\n\u001b[0;32m     47\u001b[0m pct \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(pred_pcts)\n",
      "Input \u001b[1;32mIn [54]\u001b[0m, in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, lambda_, degree, gamma, function, max_iters)\u001b[0m\n\u001b[0;32m     38\u001b[0m     w, _ \u001b[38;5;241m=\u001b[39m least_squares(y_tr, x_tr)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m function \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogisticRegression\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 42\u001b[0m     w, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlogistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m function \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRegLogisticRegression\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     46\u001b[0m     w, _ \u001b[38;5;241m=\u001b[39m reg_logistic_regression(y_tr, x_tr, lambda_, initial_w, max_iters, gamma)\n",
      "File \u001b[1;32m~\\Desktop\\GitHub\\ML_project1\\implementation.py:121\u001b[0m, in \u001b[0;36mlogistic_regression\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m    120\u001b[0m     loss \u001b[38;5;241m=\u001b[39m lr_calculate_loss(y,tx,w)\n\u001b[1;32m--> 121\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[43mlr_calculate_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m-\u001b[39m gamma\u001b[38;5;241m*\u001b[39mg\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m w, loss\n",
      "File \u001b[1;32m~\\Desktop\\GitHub\\ML_project1\\model_helpers.py:130\u001b[0m, in \u001b[0;36mlr_calculate_gradient\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03mCompute the logistic gradient\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m \n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m N \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (tx\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(sigmoid(\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m-\u001b[39my)) \u001b[38;5;241m/\u001b[39m N\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "functions = ['LinearRegression', 'RidgeRegression', 'LeastSquares', \\\n",
    "             'LogisticRegression', 'RegLogisticRegression']\n",
    "\n",
    "for func in functions[1:]:\n",
    "    cross_validation_grid_search(x_tr_subsamples,y_tr_subsamples,func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(txs, ys, lambda_, gamma, degree):\n",
    "    \"\"\"Trains the classifier model\n",
    "    \n",
    "    Args:\n",
    "        txs: training data split into three subsets\n",
    "        y: labels of training data split into three subsets\n",
    "    \n",
    "    Returns:\n",
    "        ws: weights of each subsets.\n",
    "    \"\"\"\n",
    "    \n",
    "    ws = []\n",
    "    \n",
    "    for i in range(len(txs)):\n",
    "        x_poly = build_poly(txs[i], degree[i])\n",
    "        initial_w = np.zeros((x_poly.shape[1], 1))\n",
    "        ws.append(ridge_regression(ys[i], x_poly, lambda_=lambda_[i])[0])\n",
    "        \n",
    "    return ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_, gamma, degree = [1e-4, 1e-5, 1e-5, 1e-3], [1e-4, 1e-4, 1e-4, 1e-4], [6, 4, 6, 6]\n",
    "\n",
    "ws = train_model(x_tr_subsamples, y_tr_subsamples, lambda_, gamma, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(txs_te, ws, mask_test, y_pred, degree):\n",
    "    \"\"\"Generate the predictions and save ouput\n",
    "    \n",
    "    Args:\n",
    "        txs_te: subsets of test dataset\n",
    "        ws: weights of the different subsets\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    for j in range(len(txs_te)):\n",
    "            y_pred[mask_test[j]] = [y[0] for y in predict_labels(build_poly(txs_te[j],degree[j]), ws[j])]\n",
    "            \n",
    "    create_csv_submission(ids_te, y_pred, paths['submission'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_predictions(x_te_subsamples, ws, mask_te, y_pred, degree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
