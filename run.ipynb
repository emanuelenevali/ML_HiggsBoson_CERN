{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higgs Boson - ML Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from implementation import *\n",
    "from model_helpers import *\n",
    "from data_helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = { \n",
    "    'train' : 'data/train.csv',\n",
    "     'test' : 'data/test.csv',\n",
    "     'submission' : 'data/sample-submission.csv'\n",
    "        }\n",
    "\n",
    "y_tr, tx_tr, ids_tr = load_csv_data(paths['train'], sub_sample=False)\n",
    "y_te, tx_te, ids_te = load_csv_data(paths['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_test = len(y_te)\n",
    "\n",
    "y_tr = y_tr[:, np.newaxis]\n",
    "y_pred = np.zeros(len_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into 4 different subsets depending on jet value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_tr = get_mask(tx_tr)\n",
    "mask_te = get_mask(tx_te)\n",
    "\n",
    "x_tr_subsamples = []\n",
    "y_tr_subsamples = []\n",
    "\n",
    "x_te_subsamples = []\n",
    "\n",
    "for i in range(4):\n",
    "    x_tr_subsamples.append(tx_tr[mask_tr[i]])\n",
    "    y_tr_subsamples.append(y_tr[mask_tr[i]])\n",
    "    x_te_subsamples.append(tx_te[mask_te[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(4):\n",
    "    x_tr_subsamples[j] = pre_processing(x_tr_subsamples[j], j)\n",
    "    x_te_subsamples[j] = pre_processing(x_te_subsamples[j], j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree, gamma, function, max_iters=1000):\n",
    "    \"\"\"\n",
    "    Return the loss of ridge regression for a fold corresponding to k_indices\n",
    "    \n",
    "    Args:\n",
    "        y:          shape=(N, 1)\n",
    "        x:          shape=(N, D)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold\n",
    "        lambda_:    scalar, used by ridge regression\n",
    "        degree:     scalar, used by build poly\n",
    "        gamma:      scalar, stepsize\n",
    "\n",
    "    Returns:\n",
    "        test loss: probability of predicting correct values\n",
    "    \"\"\"\n",
    "    \n",
    "    train_id = np.delete(k_indices, k, axis=0).ravel()\n",
    "    test_id = k_indices[k]\n",
    "    \n",
    "    x_tr, y_tr = x[train_id], y[train_id]\n",
    "    x_te, y_te = x[test_id], y[test_id]\n",
    "    \n",
    "    x_tr, x_te = build_poly(x_tr, degree), build_poly(x_te, degree)\n",
    "    \n",
    "    initial_w = np.zeros((x_tr.shape[1], 1))\n",
    "    \n",
    "    \n",
    "    if function == 'RidgeRegression':\n",
    "        \n",
    "        w, _ = ridge_regression(y_tr, x_tr, lambda_)\n",
    "    \n",
    "    elif function == 'LeastSquares':\n",
    "        \n",
    "        w, _ = least_squares(y_tr, x_tr)\n",
    "        \n",
    "    elif function == 'LogisticRegression':\n",
    "        \n",
    "        w, _ = logistic_regression(y_tr, x_tr, initial_w, max_iters, gamma)\n",
    "        \n",
    "    elif function == 'RegLogisticRegression':\n",
    "        \n",
    "        w, _ = reg_logistic_regression(y_tr, x_tr, lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    \n",
    "    return (y_te == predict_labels(x_te, w)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_grid_search(txs, ys, func):\n",
    "    \"\"\"\n",
    "    Runs cross validation on the data with different values of hyperparameters to compare accuracy\n",
    "    \n",
    "    Args:\n",
    "        txs: subsets of train dataset\n",
    "        ys:  labels of the different subsets\n",
    "        func: string, types of function\n",
    "        \n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    \n",
    "    seed = 51\n",
    "    k_fold = 4\n",
    "    \n",
    "    # Lambda: regularization parameter\n",
    "    lambdas = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "    \n",
    "    # Degree: feature augmentation\n",
    "    degrees = range(1, 8, 1)\n",
    "    \n",
    "    # Gamma: stepsize\n",
    "    gammas = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = []\n",
    "    for i in range(len(txs)):\n",
    "        k_indices.append(build_k_indices(ys[i].shape[0], k_fold, seed))\n",
    "        \n",
    "    print(f\"Function: {function}\")\n",
    "        \n",
    "    best_tuple = [(-1, -1, -1)]*4\n",
    "    \n",
    "    # cross validation\n",
    "    for i in range(len(txs)):\n",
    "        \n",
    "        max_acc = 0\n",
    "        print(f\"->Subset {i}:\")\n",
    "        \n",
    "        for l in lambdas:\n",
    "            for d in degrees:\n",
    "                for g in gammas:\n",
    "                    \n",
    "                    pred_pcts = []\n",
    "                    \n",
    "                    for k in range(k_fold):\n",
    "                        pred_pct = cross_validation(ys[i], txs[i], k_indices[i], k, l, d, g, func)\n",
    "                        pred_pcts.append(pred_pct)\n",
    "                        \n",
    "                    pct = np.mean(pred_pcts)\n",
    "                    if pct > max_acc:\n",
    "                        max_acc = pct\n",
    "                        best_tuple[i] = (l,d,g)\n",
    "                        print(f\"- Set {i}/lamdba={l}/deg={d}/gamma={g}/ACC={np.around(pct, 3)}\")\n",
    "                        \n",
    "    return best_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = ['RidgeRegression',    'LeastSquares', \\\n",
    "             'LogisticRegression', 'RegLogisticRegression']\n",
    "\n",
    "best_tuple = {}\n",
    "\n",
    "for func in functions:\n",
    "    best_tuple[func] = cross_validation_grid_search(x_tr_subsamples,y_tr_subsamples,func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(txs, ys, params):\n",
    "    \"\"\"Trains the classifier model\n",
    "    \n",
    "    Args:\n",
    "        txs: training data split into three subsets\n",
    "        y: labels of training data split into three subsets\n",
    "    \n",
    "    Returns:\n",
    "        ws: weights of each subsets.\n",
    "    \"\"\"\n",
    "    \n",
    "    ws = []\n",
    "    \n",
    "    for i in range(len(txs)):\n",
    "        \n",
    "        lambda_, degree, gamma = params[i]\n",
    "        x_poly = build_poly(txs[i], degree)\n",
    "        initial_w = np.zeros((x_poly.shape[1], 1))\n",
    "        \n",
    "        ws.append(ridge_regression(ys[i], x_poly, lambda_=lambda_)[0])\n",
    "        \n",
    "    return ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = [(1e-4,6,1e-4),(1e-5,4,1e-5),(1e-5,6,1e-4),(1e-3,6,1e-4)]\n",
    "\n",
    "chosen_function = 'RidgeRegression'\n",
    "\n",
    "ws = train_model(x_tr_subsamples, y_tr_subsamples, best_tuple[chosen_function])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(txs_te, ws, mask_test, y_pred, degree):\n",
    "    \"\"\"Generate the predictions and save ouput\n",
    "    \n",
    "    Args:\n",
    "        txs_te: subsets of test dataset\n",
    "        ws: weights of the different subsets\n",
    "    \"\"\"\n",
    "    \n",
    "    for j in range(len(txs_te)):\n",
    "            y_pred[mask_test[j]] = [y[0] for y in predict_labels(build_poly(txs_te[j],degree[j]), ws[j])]\n",
    "            \n",
    "    create_csv_submission(ids_te, y_pred, paths['submission'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_predictions(x_te_subsamples, ws, mask_te, y_pred, best_tuple[chosen_function][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
